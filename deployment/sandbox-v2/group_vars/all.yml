
---
# Global variables - accessed by different roles and plays
#
tmp_dir: '/tmp/' 
logs_dir: '/tmp/'

reporting_user: 'mosipdocs@gmail.com'
reporting_user_secret: ''

sandbox_domain_name: test-sandbox.southindia.cloudapp.azure.com 
site:
  sandbox_public_url: 'https://{{sandbox_domain_name}}'
  ssl:
    get_certificate: true  # get a fresh certificate for the domain using Letsencrypt.
    email: info@mosip.io
    certificate: '/etc/letsencrypt/live/{{sandbox_domain_name}}/fullchain.pem'
    certificate_key: '/etc/letsencrypt/live/{{sandbox_domain_name}}/privkey.pem'
  captcha:
    google_recaptcha_site_key: mysitekey
    google_recaptcha_secret_key: mysecretekey
  smtp:
    email_from: mosiptestuser@gmail.com
    host: smtp.sendgrid.net
    username: apikey
    password: password
   
install_root: /home/mosipuser/mosip-infra/deployment/sandbox-v2
charts_root: '{{install_root}}/helm/charts'  # Helm charts root

is_glowroot: absent  # or present

artifactory_uri: 'http://13.71.87.138:8040'

docker_wait_time: 600   # Wait for docker to pull and deploy

clusters:
  mz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/mzcluster.config" 
    nodePortNode: mzworker0  # Any node on cluster for nodeport access
    ingress:
      nodeport_http: 30080  # TODO: Shoud match with helm
      nodeport_https: 30443
      base_url: 'http://{{groups["mzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard:
      url: /mz-dashboard
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/mz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/mz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/mz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/mz/grafana'
      grafana_ingress_path: 'mz-grafana'
  dmz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/dmzcluster.config" 
    nodePortNode: dmzworker0  # Any node on cluster for nodeport access
    ingress:
      nodeport_http: 30080  # TODO: Shoud match with helm
      nodeport_https: 30443
      base_url: 'http://{{groups["dmzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard: 
      url: /dmz-dashboard
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/dmz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/dmz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/dmz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/dmz/grafana'
      grafana_ingress_path: 'dmz-grafana'

# Registration processor shared variables
#
regproc:
  nfs:
    server: '{{nfs.server}}'
    landing_folder: '{{nfs.folder}}/landing'

# Postgres persistent storage
postgres:
  nfs_path: '{{nfs.folder}}/postgres'  # TODO: This should match with helm charts
  size: 10Gi
  service_name: postgres # svc on Kubernetes
  service_port: 80
  server: mzworker0 # Nodeport
  local_port: 30090  # fixed for connecting from outside the cluster
  user: postgres
  password: postgres
  max_connections: 1000

# SFTP: used for picking up packets from DMZ
sftp_user: 'sftpuser'  # This should be same in config files

# Keycloak
keycloak:
  user: admin
  password: admin
  url: '{{clusters.mz.ingress.base_url}}/keycloak'

softhsm:
  keymanager:  
    label: keymanager  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/keymanager/'
  ida:  # Auth
    label: ida  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/ida/'

repos:
  commons:
    src: https://github.com/mosip/commons 
    dest: '{{tmp_dir}}/commons'
    tag: 1.0.9
  prereg:
    src: https://github.com/mosip/pre-registration
    dest: '{{tmp_dir}}/pre-registration'
    tag: master
  regproc:
    src: https://github.com/mosip/registration
    dest: '{{tmp_dir}}/registration'
    tag: master
  ida:
    src: https://github.com/mosip/id-authentication
    dest: '{{tmp_dir}}/id-authentication'
    tag: 1.0.9
  pms:
    src: https://github.com/mosip/partner-management-services
    dest: '{{tmp_dir}}/partner-management-services'
    tag: master

# DBs 
#
databases:
  mosip_kernel:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_master:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_iam:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_audit:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_idrepo:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_idmap:
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_prereg:
    sql_path: '{{repos.prereg.dest}}/db_scripts'
  mosip_regprc:
    sql_path: '{{repos.regproc.dest}}/db_scripts'
  mosip_ida:
    sql_path: '{{repos.ida.dest}}/db_scripts'
  mosip_pmp:
    sql_path: '{{repos.pms.dest}}/db_scripts'

nfs:
  server: console
  folder: /srv/nfs/mosip


hdfs:
  install_name: hadoop
  users:  # Must match names in property files
    - regprocessor
    - prereg
    - idrepo 
  user_base_dir: /user
  namenode_pod_name: 'hadoop-hdfs-nn-0'  # Must match name generated by helm templates
  nfs:
    server: '{{nfs.server}}'
    nn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/nn'  # Must match helm
    dn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/dn'  # Must match helm

config_repo:
  name: config_repo  # Git repo inside above node_path
  path: '{{nfs.folder}}/config_repo'
  search_folder: ''  # If all properites at at the root repo level, then this will be empt
  storage_size: 10Mi

