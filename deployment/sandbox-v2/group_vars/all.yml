---
# Global variables - accessed by different roles and plays
#
user_home: '{{lookup("env", "HOME")}}'
install_root: '{{user_home}}/mosip-infra/deployment/sandbox-v2'
tmp_dir: '{{install_root}}/tmp/'
logs_dir: '{{tmp_dir}}/logs'
tmp_yamls: '{{tmp_dir}}/yaml'
charts_root: '{{install_root}}/helm/charts'  # Helm charts root
helm_cli_path: '{{user_home}}/bin'  # This path chosen as it is included in default $PATH in Centos 7.7
artifactory_url: http://artifactory-service.default:80/
artifactory_lts_url: http://artifactory-service.lts:80/
versions_file: '{{install_root}}/versions.yml' 
podconfig_file: '{{install_root}}/podconfig.yml'
helm_args: ''

sandbox_domain_name: minibox.mosip.net
internal_domain_name: '{{ansible_host}}'  # IP address of console
site:
  sandbox_public_url: 'https://{{sandbox_domain_name}}'  # The public url needs to point to nginx on console
  sandbox_internal_url: 'https://{{internal_domain_name}}'  # Pointing to nginx which runs on console
  sandbox_internal_tld: 'sb'  # Top-level domain name.  This much match tld of all hosts in hosts.ini
  ingress_tld: 'ingress'  # Fixed. Don't change this!
  ssl:
    ca: 'letsencrypt'   # The ca to be used in this deployment

# Paths of various supported ssl certs
ssl:
  ca:  # Certifcation authority
    selfsigned: 
      get_certificate: true
      common_name: '{{internal_domain_name}}'  # The host on which certs are generated   
      certificate: '/etc/ssl/certs/nginx-selfsigned.crt'
      certificate_key: '/etc/ssl/private/nginx-selfsigned.key'
      csr: '/etc/ssl/csr/nginx-selfsigned.csr'
    letsencrypt:
      get_certificate: true  # get a fresh certificate for the domain using Letsencrypt
      email: info@mosip.io
      certificate: '/etc/letsencrypt/live/{{sandbox_domain_name}}/fullchain.pem'
      certificate_key: '/etc/letsencrypt/live/{{sandbox_domain_name}}/privkey.pem'

developer_mode: true  # false if limited APIs are exposed externally.
    
is_glowroot: absent  # absent or present

docker:
  hub:
    private: false  # For private repo on Docker Hub, set this to true, and set credentials in secrets.yml
    keyname: dockerhubkey  # Name of kubernetes secret
  local_registry: 
    enabled: false
    image: 'registry:2'
    name: 'local-registry'
    port: 5000
  # List of local docker registries (other than Docker Hub) is needed to make sure http access works when dockers
  # are pulled to k8s cluster.  Used while installing clusters. See k8s/docker role.
  registries:  
    - '{{inventory_hostname}}:5000'   # Docker registry running on console. Port should be same as above
  log_roll:
      max_size: "50m"  # See https://docs.docker.com/config/containers/logging/configure/
      max_files: 3  # Max number of rolling files

clusters:
  mz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/mzcluster.config" 
    nodeport_node: '{{groups.mzworkers[0]}}'  # Any node on cluster for nodeport access
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      node_alias: "{{'mz.' + site.ingress_tld}}" # Hardcoded, mapped to one of the nodes below in coredns config
      base_url: 'http://{{groups.mzworkers[0]}}:30080' # Any node since ingress runs on nodeport
    dashboard:
      url: /mz-dashboard
      token_expiry: 86400 # Seconds
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/mz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/mz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/mz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/mz/grafana'
      grafana_ingress_path: 'mz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_mz.token'
      grafana:
        alerts:
          # channel name of slack for sending alerts, example:
          channel: prometheus_alerts
          #webhook url of the same slack channel, example:
          url: https://hooks.slack.com/services/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
      elasticsearch:
        host: 'elasticsearch-master:9200'
      kibana: 
        url: http://kibana-kibana:5601 
    kafka:
      nfs_path: '{{nfs.folder}}/mz/kafka'  
      size: 8Gi 
      log_retention_bytes: _1073741824
      default_replication_factor: 3
      offsets_topic_replication_factor: 3
      transaction_state_log_replication_factor: 3
      num_partitions: 20
      replica_count: 5
      zookeeper_replica_count: 5
      zookeeper_size: 2Gi
      service_name: 'kafka.default:9092' 
  dmz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/dmzcluster.config" 
    nodeport_node: '{{groups.dmzworkers[0]}}'  # Any node on cluster for nodeport access
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      node_alias: "{{'dmz.' + site.ingress_tld}}" # Hardcoded, mapped to one of the nodes below in coredns config
      base_url: 'http://{{groups["dmzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard: 
      url: /dmz-dashboard
      token_expiry: 86400 # Seconds 
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/dmz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/dmz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/dmz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/dmz/grafana'
      grafana_ingress_path: 'dmz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_dmz.token'
      elasticsearch:
        host: '{{groups.mzworkers[0]}}:30080/elasticsearch' # Connect to elasticsearch on MZ
      kibana: 
        url: 'http://{{groups.mzworkers[0]}}:30601'  # Kibana runs on this nodeport on MZ.
    kafka:
      nfs_path: '{{nfs.folder}}/dmz/kafka'  
      size: 8Gi 
      log_retention_bytes: _1073741824
      default_replication_factor: 3
      offsets_topic_replication_factor: 3
      transaction_state_log_replication_factor: 3
      num_partitions: 20
      replica_count: 5
      zookeeper_replica_count: 5
      zookeeper_size: 2Gi
      service_name: 'kafka.default:9092' 

# Registration processor shared variables
#
regproc:
  nfs:
    server: '{{nfs.server}}'
    landing_folder: '{{nfs.folder}}/landing'

# Postgres persistent storage
postgres:
  external: false  # Postgres installed outside cluster, and not installed using these scripts. Set to true if
                   # you already have postgres running on a separate system.
  # Change host here if external: true
  host: '{{clusters.mz.nodeport_node}}'  # Host name of any node since standard sandbox runs postgres as pod on nodeport
  port: 30090 # hardcoded node port for postgres  
  user: postgres 
  password: '{{secrets.postgres.su}}'
  init: true  # Initialize db. Set it to false in production after initializing once.
  upgrade:
    set: false  # accepts true or false value as per need.
    version: 1.1.5 # version gto which upgrade is needed.
    type: release #should have two values release or revoke as per need.
    info_file: '{{tmp_dir}}/upgrade.db'

  # All below configs applicable only when external: false 
  internal_host: postgres # Service name of postgres k8s service. Not applicable if external=true
  internal_port: 80  # Port of the service
  nfs_path: '{{nfs.folder}}/postgres'  
  size: 10Gi
  max_connections: 1000
  node_affinity: 
    enabled: false # To run postgres on an exclusive node
    node: '{{groups.mzworkers[0]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "postgres" # Key for applying taint on node
      value: "only"  

# minio
minio:
  access_key: admin
  secret_key: '{{secrets.minio.secretkey}}'
  nfs_path: '{{nfs.folder}}/minio'
  nodeport: 32000
  node_affinity: 
    enabled: false # To run minio on an exclusive node
    node: '{{groups.mzworkers[1]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "minio" # Key for applying taint on node
      value: "only"  

# Keycloak
keycloak:
  user: admin
  password: '{{secrets.keycloak.password}}'
  url: '{{clusters.mz.ingress.base_url}}/keycloak'
  external_url: '{{ site.sandbox_public_url }}/keycloak'
  db:  # Assumed postgres 
    name: 'keycloak'  
    host: '{{postgres.host}}'  
    port: '{{postgres.port}}' 
    user: '{{postgres.user}}'
    password: '{{secrets.postgres.su}}' 
 
hsm_client_zip_path: 'artifactory/libs-release-local/hsm/client.zip' # Replace this with any other HSM's zip.
softhsm:
  keymanager:  
    label: keymanager  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/keymanager/'
  ida:  # Auth
    label: ida  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/ida/'
  esignet:
    label: esignet
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens'
    nfs_path: '{{nfs.folder}}/softhsm/esignet/'
  mock_relying_party:
    label: mock-relying-party
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens'
    nfs_path: '{{nfs.folder}}/softhsm/mock-relying-party/'
reg_client:
  cert_file: mosip_cer.cer
  nfs_path: '{{nfs.folder}}/reg-client/'
  cert_path: '{{install_root}}/roles/reg-client-prep/templates/'
  xml_file: '{{install_root}}/roles/reg-client-prep/templates/maven-metadata.xml'
  crypto_key: 'bBQX230Wskq6XpoZ1c+Ep1D+znxfT89NxLQ7P4KFkc4='
  bootpwd: 'bW9zaXAxMjM0NQ=='

mosip_file_server:
  nfs_path: '{{nfs.folder}}/mosip-file-server/'
  mosipvc_path: '{{nfs.folder}}/mosip-file-server/.well-known/mosipvc/'
  mobileapp_path: '{{nfs.folder}}/mosip-file-server/files/mobileapp/'
  tmp_dir: '{{tmp_dir}}/mosip_file_server'

repos:
  commons:
    src: https://github.com/mosip/commons
    dest: '{{tmp_dir}}/commons'
    tag: 1.1.5.5
    githubuser: ''
    githubpassword: ''
  prereg:
    src: https://github.com/mosip/pre-registration
    dest: '{{tmp_dir}}/pre-registration'
    tag: 1.1.5
    githubuser: ''
    githubpassword: ''
  regproc:
    src: https://github.com/mosip/registration
    dest: '{{tmp_dir}}/registration'
    tag: 1.1.5
    githubuser: ''
    githubpassword: ''
  idrepo:
    src: https://github.com/mosip/id-repository
    dest: '{{tmp_dir}}/id-repository'
    tag: 1.1.5.5
    githubuser: ''
    githubpassword: ''
  pms:
    src: https://github.com/mosip/partner-management-services
    dest: '{{tmp_dir}}/partner-management-services'
    tag: 1.1.5
    githubuser: ''
    githubpassword: ''
  websub:
    src: https://github.com/mosip/websub
    dest: '{{tmp_dir}}/websub'
    tag: 1.1.5
    githubuser: ''
    githubpassword: ''
  ref_impl:
    src: https://github.com/mosip/mosip-ref-impl
    dest: '{{tmp_dir}}/mosip-ref-impl'
    tag: 1.1.5
    githubuser: ''
    githubpassword: ''


# DBs 
# Please uncomment the release_path key in each of the database section if there is change in the DB in specific version
databases:
  mosip_kernel:
    user: kerneluser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_master:
    user: masteruser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_iam:
    user: iamuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_audit:
    user: audituser
    sql_path: '{{repos.commons.dest}}/db_scripts'
#    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_authdevice:
    user: authdeviceuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_keymgr:
    user: keymgruser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_regdevice:
    user: regdeviceuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.commons.dest}}/db_release_scripts'
  mosip_credential:
    user: credentialuser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
    release_path: '{{repos.idrepo.dest}}/db_release_scripts'
  mosip_idrepo:
    user: idrepouser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
    release_path: '{{repos.idrepo.dest}}/db_release_scripts'
  mosip_idmap:
    user: idmapuser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
    release_path: '{{repos.idrepo.dest}}/db_release_scripts'
  mosip_prereg:
    user: prereguser
    sql_path: '{{repos.prereg.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
#    release_path: '{{repos.prereg.dest}}/db_release_scripts'
  mosip_regprc:
    user: regprcuser
    sql_path: '{{repos.regproc.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.regproc.dest}}/db_release_scripts'
  mosip_pms:
    user: pmpuser
    sql_path: '{{repos.pms.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
    release_path: '{{repos.pms.dest}}/db_release_scripts'
  mosip_websub:
    user: websubuser
    sql_path: '{{repos.websub.dest}}/db_scripts'
    release_path: '{{repos.websub.dest}}/db_release_scripts'

nfs:
  server: '{{inventory_hostname}}'
  folder: /srv/nfs/mosip
  provisioner: nfs-provisioner  # Name

hdfs:
  install_name: hadoop
  users:  # Must match names in property files
    - regprocessor
    - prereg
    - idrepo 
  user_base_dir: /user
  namenode_pod_name: 'hadoop-hdfs-nn-0'  
  nfs:
    server: '{{nfs.server}}'
    nn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/nn'  
    dn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/dn' 
  node_affinity: 
    enabled: false # To run all hdfs pods exclusively on a single node.
    node: '{{groups.mzworkers[1]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "hdfs" # Key for applying taint on node
      value: "only"  

config_repo:
  # If local_git_repo=true, a copy of git repo at `git_repo_uri` gets created on console machine and 
  # config server points to the same. Any changes done on local repo and will not  get reflected in 
  # the parent git repo. 
  git_repo_uri: https://github.com/mosip/mosip-config
  version: 1.1.5.5
  private: false #  true if repo is private
  gitusername: '' # if private==true set username here and password in secrets as below
  gitpassword: '{{secrets.config_repo.password}}'
  search_folders: sandbox # Subdir in repo containing properties
  local_git_repo:
    enabled: false
    path: '{{nfs.folder}}/mosip-config'
    storage_size: 10Mi

config_server:
  name: config-server
  internal_url: http://config-server.default:80/config
  ingress_url: '{{clusters.mz.ingress.base_url}}/config'
  encrypt_key: '{{secrets.config_server.encrypt_key}}'

activemq:
  name: activemq
  nodeport: 30616  # Arbitrary
  container_port: 61616
  nfs_path: '{{nfs.folder}}/activemq/'
  admin_password: '{{secrets.activemq.admin}}'

resident_app_server:
  nfs_path: '{{nfs.folder}}/resident-app-server/'
  
# Private DNS - typically required for on-prem setups, but we enable it as
# default for cloud as well.  So we don't have to use cloud DNS.

coredns:
  enabled: true  # Disable to use Cloud provided DNS
  name: coredns
  path: '{{tmp_dir}}/dns'
  db: '{{tmp_dir}}/dns/sandbox_dns.db'
  ingress_db: '{{tmp_dir}}/dns/ingress_dns.db' # Separate fixed, hardcoded file for ingress
  image: coredns/coredns
  aliases: # This is to give fixed name to ingress on mz and dmz, so that we don't have to change the names in installs.
     mzingress:
         alias: '{{clusters.mz.ingress.node_alias}}'  
         node: '{{hostvars[groups.mzworkers[0]]["ansible_host"]}}' # IP of any worker node
     dmzingress:
         alias: '{{clusters.dmz.ingress.node_alias}}'  
         node: '{{hostvars[groups.dmzworkers[0]]["ansible_host"]}}' # IP of any worker node

iam_adapter_lts_url: http://artifactory-service.lts:80/artifactory/libs-release-local/io/mosip/kernel/kernel-auth-adapter.jar 

iam_adapter_url: http://artifactory-service.default:80/artifactory/libs-release-local/io/mosip/kernel/kernel-auth-adapter.jar
#The below auth adaptor is build with vertx 3.9.1 to be compatible with regproc stages and services
iam_adapter_regproc_url: 'http://artifactory-service.default:80/artifactory/libs-snapshot-local/io/mosip/kernel/kernel-auth-adapter/1.2.0-SNAPSHOT/kernel-auth-adapter-1.2.0-SNAPSHOT-regproc-20201209055103.jar'
iam_adapter_regproc_ext_url: '{{clusters.mz.ingress.base_url}}/artifactory/libs-snapshot-local/io/mosip/kernel/kernel-auth-adapter/1.2.0-SNAPSHOT/kernel-auth-adapter-1.2.0-SNAPSHOT-regproc-20201209055103.jar'
runtimeDepUrlpath: http://artifactory-service.default:80/artifactory/libs-release-local/io/mosip/kernel/
biosdk_service_url: http://mock-biosdk-service/biosdk-service
cache_provider_url: http://artifactory-service.default:80/artifactory/libs-release-local/cache/cache-provider.jar

# user for every service inside the container
container_user: mosip


db_modules: '["mosip_master", "mosip_kernel", "mosip_iam", "mosip_audit", "mosip_authdevice", "mosip_keymgr", "mosip_regdevice", "mosip_credential", "mosip_idrepo", "mosip_idmap", "mosip_prereg", "mosip_regprc", "mosip_ida", "mosip_pms", "mosip_websub", "mosip_hotlist"]'
backup_path: '{{tmp_dir}}/db-backup/'
