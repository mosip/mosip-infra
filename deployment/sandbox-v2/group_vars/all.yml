
---
# Global variables - accessed by different roles and plays
#
user_home: '{{lookup("env", "HOME")}}'
install_root: '{{user_home}}/mosip-infra/deployment/sandbox-v2'
tmp_dir: '{{install_root}}/tmp/'
logs_dir: '{{tmp_dir}}/logs'
charts_root: '{{install_root}}/helm/charts'  # Helm charts root
helm_cli_path: '{{user_home}}/bin'  # This path chosen as it is included in default $PATH in Centos 7.7
artifactory_url: http://artifactory-service/
versions_file: '{{install_root}}/versions.yml' 
podconfig_file: '{{install_root}}/podconfig.yml'

sandbox_domain_name: minibox.mosip.net
site:
  sandbox_public_url: 'https://{{sandbox_domain_name}}'  # The public url needs to point to nginx on console
  sandbox_internal_url: 'https://{{inventory_hostname}}'  # Pointing to nginx which runs on console
  sandbox_internal_tld: 'sb'  # Top-level domain name.  This much match tld of all hosts in hosts.ini
  ssl:
    ca: 'letsencrypt'   # The ca to be used in this deployment

# Paths of various supported ssl certs
ssl:
  ca:  # Certifcation authority
    selfsigned: 
      get_certificate: true
      common_name: '{{ansible_host}}'  # The host on which certs are generated   
      certificate: '/etc/ssl/certs/nginx-selfsigned.crt'
      certificate_key: '/etc/ssl/private/nginx-selfsigned.key'
      csr: '/etc/ssl/csr/nginx-selfsigned.csr'
    letsencrypt:
      get_certificate: true  # get a fresh certificate for the domain using Letsencrypt
      email: info@mosip.io
      certificate: '/etc/letsencrypt/live/{{sandbox_domain_name}}/fullchain.pem'
      certificate_key: '/etc/letsencrypt/live/{{sandbox_domain_name}}/privkey.pem'

developer_mode: true  # false if limited APIs are exposed externally.
    
is_glowroot: absent  # absent or present

docker_wait_time: 600   # Wait for docker to pull and deploy
docker_hub:
  private: false  # For private repo on Docker Hub, set this to true, and set credentials in secrets.yml
  keyname: dockerhubkey  # Name of kubernetes secret

local_docker_registry:
  enabled: false
  image: 'registry:2'
  name: 'local-registry'
  port: 5000
  
# List of local docker registries (other than Docker Hub) is needed to make sure http access works when dockers
# are pulled from k8s cluster.  Used while installing clusters. See k8s/docker role.
docker_registries:  
  - '{{inventory_hostname}}:{{local_docker_registry.port}}'   # Docker registry running on console

clusters:
  mz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/mzcluster.config" 
    nodeport_node: '{{groups.mzworkers[0]}}'  # Any node on cluster for nodeport access
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      node_alias: 'mzingress.{{site.sandbox_internal_tld}}' # This is mapped to below node in coredns
      base_url: 'http://{{groups.mzworkers[0]}}:30080' # Any node since ingress runs on nodeport
    dashboard:
      url: /mz-dashboard
      token_file: '{{tmp_dir}}/dashboard_mz.token'
      token_expiry: 86400 # Seconds
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/mz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/mz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/mz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/mz/grafana'
      grafana_ingress_path: 'mz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_mz.token'
      elasticsearch:
        host: 'elasticsearch-master:9200'
      kibana: 
        url: http://kibana-kibana:5601 
    kafka:
      nfs_path: '{{nfs.folder}}/mz/kafka'  
      size: 8Gi 
      log_retention_bytes: _1073741824
      default_replication_factor: 3
      offsets_topic_replication_factor: 3
      transaction_state_log_replication_factor: 3
      num_partitions: 250
      replica_count: 5
      zookeeper_replica_count: 5
      zookeeper_size: 2Gi 
  dmz:
    kube_config:  "{{lookup('env', 'HOME') }}/.kube/dmzcluster.config" 
    nodeport_node: '{{groups.dmzworkers[0]}}'  # Any node on cluster for nodeport access
    ingress:
      namespace: ingress-nginx
      nodeports:
        http: 30080 
        https: 30443
      node_alias: 'dmzingress.{{site.sandbox_internal_tld}}' # This is mapped to below node in coredns
      base_url: 'http://{{groups["dmzworkers"][0]}}:30080' # Any node since ingress runs on nodeport
    dashboard: 
      url: /dmz-dashboard
      token_file: '{{tmp_dir}}/dashboard_dmz.token'
      token_expiry: 86400 # Seconds 
      nodeport: 30081  # Dashboard runs on nodeport 
    monitoring:
      enabled: true
      namespace: monitoring
      nfs:
        server: '{{nfs.server}}'
        prometheus:
          alert_path: '{{nfs.folder}}/monitoring/dmz/prometheus/alertmanager'
          push_path: '{{nfs.folder}}/monitoring/dmz/prometheus/pushgateway'
          server_path: '{{nfs.folder}}/monitoring/dmz/prometheus/server'
        grafana:
          path: '{{nfs.folder}}/monitoring/dmz/grafana'
      grafana_ingress_path: 'dmz-grafana'
      grafana_token_file: '{{tmp_dir}}/grafana_dmz.token'
      elasticsearch:
        host: '{{groups.mzworkers[0]}}:30080/elasticsearch' # Connect to elasticsearch on MZ
      kibana: 
        url: 'http://{{groups.mzworkers[0]}}:30601'  # Kibana runs on this nodeport on MZ.
    kafka:
      nfs_path: '{{nfs.folder}}/dmz/kafka'  
      size: 8Gi 
      log_retention_bytes: _1073741824
      default_replication_factor: 3
      offsets_topic_replication_factor: 3
      transaction_state_log_replication_factor: 3
      num_partitions: 250
      replica_count: 5
      zookeeper_replica_count: 5
      zookeeper_size: 2Gi 

# Registration processor shared variables
#
regproc:
  nfs:
    server: '{{nfs.server}}'
    landing_folder: '{{nfs.folder}}/landing'

# Postgres persistent storage
postgres:
  external: false  # Postgres installed outside cluster, and not installed using these scripts. Set to true if
                   # you already have postgres running on a separate system.
  # Change host here if external: true
  host: '{{clusters.mz.nodeport_node}}'  # Any node since standard sandbox runs postgres as pod on nodeport
  port: 30090 # hardcoded node port for postgres  
  user: postgres 
  password: '{{secrets.postgres.su}}'

  # All below configs applicable only when external: false 
  internal_host: postgres # Service name of postgres k8s service. Not applicable if external=true
  internal_port: 80  # Port of the service
  nfs_path: '{{nfs.folder}}/postgres'  
  size: 10Gi
  max_connections: 1000
  node_affinity: 
    enabled: false # To run postgres on an exclusive node
    node: '{{groups.mzworkers[0]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "postgres" # Key for applying taint on node
      value: "only"  

# minio
minio:
  access_key: admin
  secret_key: '{{secrets.minio.secretkey}}'
  nfs_path: '{{nfs.folder}}/minio'
  node_affinity: 
    enabled: false # To run minio on an exclusive node
    node: '{{groups.mzworkers[1]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "minio" # Key for applying taint on node
      value: "only"  

# Keycloak
keycloak:
  user: admin
  password: '{{secrets.keycloak.password}}'
  url: '{{clusters.mz.ingress.base_url}}/keycloak'
  external_url: '{{ site.sandbox_public_url }}/keycloak'
  db:  # Assumed postgres 
    name: 'keycloak'  
    host: '{{postgres.host}}'  
    port: '{{postgres.port}}' 
    user: '{{postgres.user}}'
    password: '{{secrets.postgres.su}}' 
 
hsm_client_zip_path: 'artifactory/libs-release-local/hsm/client.zip' # Replace this with any other HSM's zip.
softhsm:
  keymanager:  
    label: keymanager  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/keymanager/'
  ida:  # Auth
    label: ida  
    conf_file: softhsm2.conf
    conf_path: '{{install_root}}/roles/softhsm-prep/files/'
    tokens_path: '{{install_root}}/roles/softhsm-prep/files/tokens' 
    nfs_path: '{{nfs.folder}}/softhsm/ida/'

reg_client:
  cert_file: mosip_cer.cer
  nfs_path: '{{nfs.folder}}/reg-client/'
  cert_path: '{{install_root}}/roles/reg-client-prep/templates/'
  xml_file: '{{install_root}}/roles/reg-client-prep/templates/maven-metadata.xml'
  crypto_key: 'bBQX230Wskq6XpoZ1c+Ep1D+znxfT89NxLQ7P4KFkc4='
  bootpwd: 'bW9zaXAxMjM0NQ=='

repos:
  commons:
    src: https://github.com/mosip/commons 
    dest: '{{tmp_dir}}/commons'
    tag: 1.1.3
  prereg:
    src: https://github.com/mosip/pre-registration
    dest: '{{tmp_dir}}/pre-registration'
    tag: 1.1.3
  regproc:
    src: https://github.com/mosip/registration
    dest: '{{tmp_dir}}/registration'
    tag: 1.1.3
  idrepo:
    src: https://github.com/mosip/id-repository
    dest: '{{tmp_dir}}/id-repository'
    tag: 1.1.3
  ida:
    src: https://github.com/mosip/id-authentication
    dest: '{{tmp_dir}}/id-authentication'
    tag: 1.1.3
  pms:
    src: https://github.com/mosip/partner-management-services
    dest: '{{tmp_dir}}/partner-management-services'
    tag: 1.1.3
  websub:
    src: https://github.com/mosip/websub
    dest: '{{tmp_dir}}/websub'
    tag: 1.1.2
  ref_impl:
    src: https://github.com/mosip/mosip-ref-impl
    dest: '{{tmp_dir}}/mosip-ref-impl'
    tag: 1.1.3

# DBs 
#
databases:
  mosip_kernel:
    user: kerneluser
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_master:
    user: masteruser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_iam:
    user: iamuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_audit:
    user: audituser
    sql_path: '{{repos.commons.dest}}/db_scripts'
  mosip_authdevice:
    user: authdeviceuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_keymgr:
    user: keymgruser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_regdevice:
    user: regdeviceuser
    sql_path: '{{repos.commons.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_credential:
    user: credentialuser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
  mosip_idrepo:
    user: idrepouser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
  mosip_idmap:
    user: idmapuser
    sql_path: '{{repos.idrepo.dest}}/db_scripts'
  mosip_prereg:
    user: prereguser
    sql_path: '{{repos.prereg.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_regprc:
    user: regprcuser
    sql_path: '{{repos.regproc.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_ida:
    user: idauser
    sql_path: '{{repos.ida.dest}}/db_scripts'
  mosip_pms:
    user: pmpuser
    sql_path: '{{repos.pms.dest}}/db_scripts'
    dml_path: '{{repos.ref_impl.dest}}/data-dml'
  mosip_websub:
    user: websubuser
    sql_path: '{{repos.websub.dest}}/db_scripts'

nfs:
  server: '{{inventory_hostname}}'
  folder: /srv/nfs/mosip
  provisioner: nfs-provisioner  # Name

hdfs:
  install_name: hadoop
  users:  # Must match names in property files
    - regprocessor
    - prereg
    - idrepo 
  user_base_dir: /user
  namenode_pod_name: 'hadoop-hdfs-nn-0'  
  nfs:
    server: '{{nfs.server}}'
    nn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/nn'  
    dn:
      size: '5Gi'
      path: '{{nfs.folder}}/hdfs/dn' 
  node_affinity: 
    enabled: false # To run all hdfs pods exclusively on a single node.
    node: '{{groups.mzworkers[1]}}' # Hostname. Run only on this node, and nothing else should run on this node
    taint:
      key: "hdfs" # Key for applying taint on node
      value: "only"  

config_repo:
  # If local_git_repo=true, a copy of git repo at `git_repo_uri` gets created on console machine and 
  # config server points to the same. Any changes done on local repo and will not  get reflected in 
  # the parent git repo.
  git_repo_uri: https://github.com/mosip/mosip-config
  version: 1.1.3
  private: false
  username: githubusername # Applicable only if private=true. Password is expected to be in vaulted secrets.yml 
  password: '{{secrets.config_repo.password}}'
  search_folders: sandbox # Subdir in repo containing properties
  local_git_repo:
    enabled: false
    path: '{{nfs.folder}}/mosip-config'
    storage_size: 10Mi

config_server:
  name: config-server
  internal_url: http://config-server/config
  ingress_url: '{{clusters.mz.ingress.base_url}}/config'
  encrypt_key: '{{secrets.config_server.encrypt_key}}'

activemq:
  name: activemq
  nodeport: 30616  # Arbitrary
  container_port: 61616

# Private DNS - typically required for on-prem setups, but we enable it as
# default for cloud as well.  So we don't have to use cloud DNS.

coredns:
  enabled: true  # Disable to use Cloud provided DNS
  name: coredns
  path: '{{tmp_dir}}/dns'
  db: '{{tmp_dir}}/dns/sandbox_dns.db'
  image: coredns/coredns
  aliases: # This is to give fixed name to ingress on mz and dmz, so that we don't have to change the names in installs.
     mzingress:
         alias: '{{clusters.mz.ingress.node_alias}}'  
         node: '{{groups.mzworkers[0]}}'  # Any node on cluster since ingress runs on nodeport
     dmzingress:
         alias: '{{clusters.dmz.ingress.node_alias}}'  
         node: '{{groups.dmzworkers[0]}}'  
 

iam_adapter_url: http://artifactory-service/artifactory/libs-release-local/io/mosip/kernel/kernel-auth-adapter.jar
#The below auth adaptor is build with vertx 3.9.1 to be compatible with regproc stages and services
iam_adapter_ext_url: '{{clusters.mz.ingress.base_url}}/artifactory/libs-release-local/io/mosip/kernel/kernel-auth-adapter-1.2.0-SNAPSHOT-regproc-20201209055103.jar'
runtimeDepUrlpath: http://artifactory-service/artifactory/libs-release-local/io/mosip/kernel/
biosdk_service_url: http://13.233.66.241/biosdk-service
